
# 2021-1-15

### error read: http://www.jintiankansha.me/rss/GEYDCOBUPRRWGMDBGFSDIZTGMM2GCZRVMQZGGNRQHBRTQZLEME4TQYLFGJRDKZBTGI2TSZJUGM======

### [ç¤¾ä¿å¡ä½•æ—¶å…¨å›½ç»Ÿä¸€ï¼Ÿäººç¤¾éƒ¨å›åº”æ¥äº†](https://m.21jingji.com/article/20210114/herald/f7adc120208595ee589f45f7633407fc.html)

### [nacos å‡ºç°ä¸¥é‡å®‰å…¨æ¼æ´ï¼Œç‰¹æ®Š UA å¯ä»¥ç»•è¿‡æ‰€æœ‰é‰´æƒï¼Œæœ‰ç”¨äº†çš„å°ä¼™ä¼´æ³¨æ„äº†â€¦â€¦](https://www.v2ex.com/t/744865)

### [é¦–æ¬¡å‘ç°éå…‰åˆç»†èŒä¸­çš„ç”Ÿç‰©é’Ÿ](http://jandan.net/p/108331)

### [çŸ¥é“ä»€ä¹ˆæ˜¯æŠ—è¥å…»ç´ å—](http://jandan.net/p/108346)

### [ä¸€ç¾ç”·å­è¡€æ¶²ä¸­é•¿å‡ºâ€œè‡´å¹»è˜‘è‡â€](http://jandan.net/p/108349)

### [ç«è½¦åƒè¶³è™«æ¯8å¹´å°±ä¼šå¸­å·æ—¥æœ¬å±±åŒº](http://jandan.net/p/108342)

### [Virtual Machine Startup Shells Closes the Digital Divide One Cloud Computer at a Time](https://www.linuxjournal.com/content/virtual-machine-startup-shells-closes-digital-divide-one-cloud-computer-time)

### [ã€OpenSocial Specificationã€‘ç½‘é¡µé“¾æ¥ å¼€æ”¾ç¤¾ä¼šè§„èŒƒã€‚ [å›¾ç‰‡]](https://weibo.com/1715118170/JDcEYxBOG)

### [ã€Lulu â€“ Mac open-source firewall that aims to block unknown outgoing connectionsã€‘ç½‘é¡µé“¾æ¥ Lulu â€“ Macå¼€æºé˜²ç«å¢™ï¼Œæ—¨åœ¨é˜»æ­¢æœªçŸ¥çš„ä¼ å‡ºè¿æ¥ã€‚ç½‘è·¯å†·çœ¼æŠ€...](https://weibo.com/1715118170/JDcsIBGDu)

### [#ç»¿æ´²æ‘„å½±#ä¸­å›½.äº‘å—.å¤§ç†.æ´±æµ·.å°æ™®é™€ ç»¿æ´² [å›¾ç‰‡]](https://weibo.com/1715118170/JDcmqeVFH)

### [ã€Superintelligence cannot be contained: Lessons from Computability Theoryã€‘ç½‘é¡µé“¾æ¥ è®º æ–‡ã€Šä¸èƒ½åŒ…å«è¶…çº§æ™ºèƒ½ï¼šå¯è®¡ç®—æ€§ç†è®ºçš„æ•™è®­ã€‹ã€‚ [å›¾ç‰‡]](https://weibo.com/1715118170/JDcgVf5qk)

### [ã€A cool Drag-and-Drop implementation for Svelteã€‘ç½‘é¡µé“¾æ¥ Svelteçš„ä¸€ä¸ªå¾ˆé…·çš„æ‹–æ”¾å®ç°ã€‚ ç½‘è·¯å†·çœ¼æŠ€æœ¯åˆ†äº« #ç§‘æŠ€æš–å¿ƒå­£# [å›¾ç‰‡]](https://weibo.com/1715118170/JDc4myjSV)

### [Recognizing Pose Similarity in Images and Videos](http://feedproxy.google.com/~r/blogspot/gJZg/~3/3l_KQibU2Bw/recognizing-pose-similarity-in-images.html)

 <span class="byline-author">Posted by Jennifer J. Sun, Student Researcher and Ting Liu, Senior Software Engineer, Google Research</span> <p>Everyday actions, such as jogging, reading a book, pouring water, or playing sports, can be viewed as a sequence of <a href="https://en.wikipedia.org/wiki/Pose_(computer_vision)">poses</a>, consisting of the position and orientation of a personâ€™s body. An understanding of poses from images and videos is a crucial step for enabling a range of applications, including <a href="https://ai.googleblog.com/2019/03/real-time-ar-self-expression-with.html">augmented reality</a> display, <a href="https://blog.google/technology/ai/move-mirror-you-move-and-80000-images-move-you/">full-body gesture control</a>, and <a href="https://ai.googleblog.com/2020/06/repnet-counting-repetitions-in-videos.html">physical exercise quantification</a>. However, a 3-dimensional pose captured in two dimensions in images and videos appears different depending on  the viewpoint of the camera. The ability to recognize similarity in 3D pose using only 2D information will help vision systems better understand the world. </p><p>In â€œ<a href="https://arxiv.org/abs/1912.01001">View-Invariant Probabilistic Embedding for Human Pose</a>â€ (Pr-VIPE), a spotlight paper at <a href="https://eccv2020.eu/">ECCV 2020</a>, we present a new algorithm for human pose perception that recognizes similarity in human body poses across different camera views by mapping <a href="https://cocodataset.org/#keypoints-2020">2D body pose keypoints</a> to a view-invariant embedding space. This ability enables tasks, such as pose retrieval, action recognition, action video synchronization, and more. Compared to <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Martinez_A_Simple_yet_ICCV_2017_paper.pdf">existing models</a> that directly map 2D pose keypoints to 3D pose keypoints, the Pr-VIPE embedding space is (1) view-invariant, (2) <a href="https://en.wikipedia.org/wiki/Normal_distribution">probabilistic</a> in order to capture 2D input ambiguity, and (3) does not require <a href="https://en.wikipedia.org/wiki/Camera_resectioning#Projection">camera parameters</a> during training or inference. Trained with in-lab setting data, the model works on in-the-wild images out of the box, given a reasonably good 2D pose estimator (e.g., <a href="https://arxiv.org/pdf/1803.08225.pdf">PersonLab</a>, <a href="https://ai.googleblog.com/2020/08/on-device-real-time-body-pose-tracking.html">BlazePose</a>, among others). The model is simple, results in compact embeddings, and can be trained (in ~1 day) using 15 CPUs. We have released the code on <a href="https://github.com/google-research/google-research/tree/master/poem">our GitHub repo</a>.  </p><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-RNUg7WWiVhI/X_91h1QzsBI/AAAAAAAAG_4/B2wCJOoi-ssxWb9rfydHk3YGUsxWA895ACLcBGAsYHQ/s640/image16.gif" style="margin-left: auto; margin-right: auto;"><img border="0" height="570" src="https://1.bp.blogspot.com/-RNUg7WWiVhI/X_91h1QzsBI/AAAAAAAAG_4/B2wCJOoi-ssxWb9rfydHk3YGUsxWA895ACLcBGAsYHQ/w640-h570/image16.gif" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Pr-VIPE can be directly applied to align videos from different views.</td></tr></tbody></table><p><b>Pr-VIPE </b><br />The input to Pr-VIPE is a set of 2D keypoints, from any 2D pose estimator that produces a minimum of <a href="https://cocodataset.org/#keypoints-2020">13 body keypoints</a>, and the output is the <a href="https://en.wikipedia.org/wiki/Normal_distribution">mean and variance</a> of the pose embedding. The distances between embeddings of 2D poses correlate to their similarities in absolute 3D pose space. Our approach is based on two observations: </p><ul><li>The same 3D pose may appear very different in 2D as the viewpoint changes. </li><li>The same 2D pose can be <a href="https://en.wikipedia.org/wiki/3D_projection#Perspective_projection">projected</a> from different 3D poses. </li></ul><p>The first observation motivates the need for view-invariance. To accomplish this, we define the <em>matching probability</em>, i.e., the likelihood that different 2D poses were projected from the same, or similar 3D poses. The matching probability predicted by Pr-VIPE for matching pose pairs should be higher than for non-matching pairs. </p><p>To address the second observation, Pr-VIPE utilizes a probabilistic embedding formulation. Because many 3D poses can project to the same or similar 2D poses, the model input exhibits an inherent ambiguity that is difficult to capture through <a href="https://en.wikipedia.org/wiki/Deterministic_algorithm">deterministic</a> mapping point-to-point in embedding space. Therefore, we map a 2D pose through a probabilistic mapping to an embedding distribution, of which we use the variance to represent the uncertainty of the input 2D pose. As an example, in the figure below the third 2D view of the 3D pose on the left is similar to the first 2D view of a different 3D pose on the right, so we map them into a similar location in the embedding space with large variances. </p><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-UxnxKO-tuG4/X_92DyZPcHI/AAAAAAAAHAA/f5p28lqJbochbIg-FU0Vj9CzvXVXzz-TwCLcBGAsYHQ/s954/image6.jpg" style="margin-left: auto; margin-right: auto;"><img border="0" height="406" src="https://1.bp.blogspot.com/-UxnxKO-tuG4/X_92DyZPcHI/AAAAAAAAHAA/f5p28lqJbochbIg-FU0Vj9CzvXVXzz-TwCLcBGAsYHQ/w640-h406/image6.jpg" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Pr-VIPE enables vision systems to recognize 2D poses across views. We embed 2D poses using Pr-VIPE such that the embeddings are (1) view-invariant (2D projections of similar 3D poses are embedded close together) and (2) probabilistic. By embedding detected 2D poses, Pr-VIPE enables direct retrieval of pose images from different views, and can also be applied to action recognition and video alignment.</td></tr></tbody></table><em>View-Invariance</em><br />During training, we use 2D poses from two sources: <a href="http://vision.imar.ro/human3.6m/description.php">multi-view images</a> and <a href="https://en.wikipedia.org/wiki/3D_projection#Perspective_projection">projections</a> of groundtruth 3D poses. <a href="https://en.wikipedia.org/wiki/Triplet_loss">Triplets</a> of 2D poses (anchor, positive, and negative) are selected from a batch, where the anchor and positive are two different projections of the same 3D pose, and the negative is a projection of a non-matching 3D pose. Pr-VIPE then estimates the matching probability of 2D pose pairs from their embeddings. <br />During training, we push the matching probability of positive pairs to be close to 1 with a positive pairwise loss in which we minimize the embedding distance between positive pairs, and the matching probability of negative pairs to be small by maximizing the ratio of the matching probabilities between positive and negative pairs with a triplet ratio loss.  <br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-jghZ4_x8VeM/X_92Pzy896I/AAAAAAAAHAE/UsuxcBVMGacJQX6UOuyfiXBD1g-aA_A8wCLcBGAsYHQ/s1254/image13.jpg" style="margin-left: auto; margin-right: auto;"><img border="0" height="230" src="https://1.bp.blogspot.com/-jghZ4_x8VeM/X_92Pzy896I/AAAAAAAAHAE/UsuxcBVMGacJQX6UOuyfiXBD1g-aA_A8wCLcBGAsYHQ/w640-h230/image13.jpg" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Overview of the Pr-VIPE model. During training, we apply three losses (triplet ratio loss, positive pairwise loss, and a prior loss that applies a unit Gaussian prior to our embeddings). During inference, the model maps an input 2D pose to a probabilistic, view-invariant embedding.</td></tr></tbody></table><em>Probabilistic Embedding</em><br />Pr-VIPE maps a 2D pose to a probabilistic embedding as a <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">multivariate Gaussian distribution using</a> a <a href="https://arxiv.org/abs/1810.00319">sampling-based approach</a> for similarity score computation between two distributions. During training, we use a Gaussian prior loss to regularize the predicted distribution. <p><b>Evaluation</b><br />We propose a new <em>cross-view pose retrieval</em> benchmark to evaluate the view-invariance property of the embedding. Given a monocular pose image, cross-view retrieval aims to retrieve the same pose from different views without using camera parameters. The results demonstrate that Pr-VIPE retrieves poses more accurately across views compared to baseline methods in both evaluated datasets (<a href="http://vision.imar.ro/human3.6m/description.php">Human3.6M</a>, <a href="http://gvv.mpi-inf.mpg.de/3dhp-dataset/">MPI-INF-3DHP</a>). </p><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-dqjpSmDUuVk/X_92X439bRI/AAAAAAAAHAM/ODgkBCGa23wYpVgwEMhQ6k_9pmkZSyv2gCLcBGAsYHQ/s1611/image9.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="290" src="https://1.bp.blogspot.com/-dqjpSmDUuVk/X_92X439bRI/AAAAAAAAHAM/ODgkBCGa23wYpVgwEMhQ6k_9pmkZSyv2gCLcBGAsYHQ/w640-h290/image9.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Pr-VIPE retrieves poses across different views more accurately relative to the baseline method (<a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Martinez_A_Simple_yet_ICCV_2017_paper.pdf">3D pose estimation</a>).</td></tr></tbody></table><p>Common 3D pose estimation methods (such as the <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Martinez_A_Simple_yet_ICCV_2017_paper.pdf">simple baseline</a> used for comparison above, <a href="https://arxiv.org/abs/1904.03345">SemGCN</a>, and <a href="https://arxiv.org/abs/1903.02330">EpipolarPose</a>, amongst many others), predict 3D poses in camera coordinates, which are not directly view-invariant. Thus, <a href="https://en.wikipedia.org/wiki/Procrustes_analysis">rigid alignment</a> between every query-index pair is required for retrieval using estimated 3D poses, which is computationally expensive due to the need for <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a> (SVD). In contrast, Pr-VIPE embeddings can be directly used for distance computation in Euclidean space, without any post-processing. </p><p><b>Applications</b><br />View-invariant pose embedding can be applied to many image and video related tasks. Below, we show Pr-VIPE applied to cross-view retrieval on in-the-wild images without using camera parameters.</p><p><i></i></p><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-Y6wz_YqHthc/X_92-Pa-qOI/AAAAAAAAHAk/ez9D_xToW_sZE2Rwio4VQot0tFeezJw9ACLcBGAsYHQ/s1181/Pr-VIPE-InTheWild.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="538" src="https://1.bp.blogspot.com/-Y6wz_YqHthc/X_92-Pa-qOI/AAAAAAAAHAk/ez9D_xToW_sZE2Rwio4VQot0tFeezJw9ACLcBGAsYHQ/w640-h538/Pr-VIPE-InTheWild.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"></td></tr></tbody></table><i><br />We can retrieve in-the-wild images from different views without using camera parameters by embedding the detected 2D pose using Pr-VIPE. Using the query image (top row), we search for a matching pose from a different camera view and we show the nearest neighbor retrieval (bottom row). This enables us to search for matching poses across camera views more easily. </i><p></p><p>The same Pr-VIPE model can also be used for video alignment. To do so, we stack Pr-VIPE embeddings within a small time window, and use the <a href="https://en.wikipedia.org/wiki/Dynamic_time_warping">dynamic time warping</a> (DTW) algorithm to align video pairs.  </p><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-4GrUP63WOvY/X_93G9b398I/AAAAAAAAHAo/zwnsATDSkGUW22to6BbZwPpToITe1QKMQCLcBGAsYHQ/s640/image1.gif" style="margin-left: auto; margin-right: auto;"><img border="0" height="570" src="https://1.bp.blogspot.com/-4GrUP63WOvY/X_93G9b398I/AAAAAAAAHAo/zwnsATDSkGUW22to6BbZwPpToITe1QKMQCLcBGAsYHQ/w640-h570/image1.gif" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Manual video alignment is difficult and time-consuming. Here, Pr-VIPE is applied to automatically align videos of the same action repeated from different views.</td></tr></tbody></table><p>The video alignment distance calculated via DTW can then be used for action recognition by classifying videos using <a href="https://en.wikipedia.org/wiki/Nearest_neighbor_search">nearest neighbor search</a>. We evaluate the Pr-VIPE embedding using the <a href="http://dreamdragon.github.io/PennAction/">Penn Action</a> dataset and demonstrate that using the Pr-VIPE embedding without fine-tuning on the target dataset, yields highly competitive recognition accuracy. In addition, we show that Pr-VIPE even achieves relatively accurate results using only videos from a single view in the index set. </p><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-ah2jdq78hmE/X_93O1awojI/AAAAAAAAHAw/BwS5Kggqo80wlD82alhQdsQiOrhiaRUDwCLcBGAsYHQ/s1127/image15.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="190" src="https://1.bp.blogspot.com/-ah2jdq78hmE/X_93O1awojI/AAAAAAAAHAw/BwS5Kggqo80wlD82alhQdsQiOrhiaRUDwCLcBGAsYHQ/w640-h190/image15.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Pr-VIPE recognizes action across views using pose inputs only, and is comparable to or better than methods using pose only or with additional context information (such as <a href="https://arxiv.org/abs/1603.04037">Iqbal et al.</a>, <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Recognizing_Human_Actions_CVPR_2018_paper.pdf">Liu and Yuan</a>, <a href="https://arxiv.org/abs/1912.08077">Luvizon et al.</a>, and <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Du_RPAN_An_End-To-End_ICCV_2017_paper.pdf">Du et al.</a>). When action labels are only available for videos from a single view, Pr-VIPE (1-view only) can still achieve relatively accurate results.</td></tr></tbody></table><p><b>Conclusion</b><br />We introduce the Pr-VIPE model for mapping 2D human poses to a view-invariant probabilistic embedding space, and show that the learned  embeddings can be directly used for pose retrieval, action recognition, and video alignment. Our cross-view retrieval benchmark can be used to test the view-invariant property of other embeddings. We look forward to hearing about what you can do with pose embeddings! </p><p><b>Acknowledgments</b><br /><em>Special thanks to Jiaping Zhao, Liang-Chieh Chen, Long Zhao (Rutgers University), Liangzhe Yuan, Yuxiao Wang, Florian Schroff, Hartwig Adam, and the Mobile Vision team for the wonderful collaboration and support.</em></p><div class="feedflare">
<a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=3l_KQibU2Bw:DI8hFWQcDuk:yIl2AUoC8zA"><img border="0" src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" /></a>
</div><img alt="" height="1" src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/3l_KQibU2Bw" width="1" />

### [æˆä¸ºè‡ªå·±ï¼Œå§šå®‰å¨œå‡ºé“çºªå½•ç‰‡ã€Œç ´æ ¼å…¬ä¸»ã€](https://app.vmovier.com/apiv3/post/view?postid=60975)

### [çŸ¥ä¹åå‘¨å¹´å“ç‰ŒçŸ­ç‰‡ã€Œæœ‰é—®é¢˜å°±ä¼šæœ‰ç­”æ¡ˆã€](https://app.vmovier.com/apiv3/post/view?postid=60973)

### [è¶…éœ‡æ’¼æ°”è±¡å»¶æ—¶ã€Œå¤©è±¡Sky-Lapse 2020ã€](https://app.vmovier.com/apiv3/post/view?postid=60971)

### [Insta360 å¹´åº¦å›é¡¾ã€ŒBest of 2020ã€](https://app.vmovier.com/apiv3/post/view?postid=60960)

### [è¡¨æƒ…åŒ…ä¹Ÿèƒ½å‡ºä¸“è¾‘ã€Œæ—¶é«¦çš„popoçŒ«ã€](https://app.vmovier.com/apiv3/post/view?postid=60964)

### [æ³°å›½å¹¿å‘Šå‘è¯äº†ã€Œçœ‹ç”µå½±è¯·å…³æœºã€](https://app.vmovier.com/apiv3/post/view?postid=60958)

### [æ—…æ‹åšä¸»çš„å£°éŸ³æ—¥è®°ã€Œæˆ‘çœ¼ä¸­çš„2020ã€](https://app.vmovier.com/apiv3/post/view?postid=60968)

### [åˆ›æ„è§†è§‰æ¬ºè¯ˆæœ¯ã€Œç”·å·«2020åˆé›†ã€](https://app.vmovier.com/apiv3/post/view?postid=60934)

### [å¤§ä¼—ä¸€é•œåˆ°åº•åˆ›æ„å¹¿å‘Šã€Œå†å²çš„è½¦è½®ã€](https://app.vmovier.com/apiv3/post/view?postid=60969)

### [ä¸­å›½è‡ªåŠ¨é©¾é©¶å‘å±•æŠ¥å‘Š2020ï¼ˆä¸Šï¼‰ï¼š æ„ŸçŸ¥ç¯‡â€”â€”æµªæ½®å·²è‡³](http://www.jintiankansha.me/t/2a24CornNi)

### error read: http://www.waerfa.com/feed

### error read: http://www.bigdatainterview.com/feed/

### [Word Ladders and Equivalence Classes](https://datagenetics.com/blog/january52021/index.html)

### [ã€A Brief History of Consumer Cultureã€‘ç½‘é¡µé“¾æ¥ æ¶ˆè´¹æ–‡åŒ–ç®€å²ã€‚åœ¨20ä¸–çºªçš„æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œèµ„æœ¬ä¸»ä¹‰é€šè¿‡å°†æ™®é€šäººå¡‘é€ æˆå¯¹æ›´å¤šä¸œè¥¿æœ‰ä¸å¯æŠ‘åˆ¶çš„æ¸´æœ›çš„æ¶ˆè´¹è€…æ¥ä¿æŒ...](https://weibo.com/1715118170/JDeeve5Ho)

### error read: http://www.jintiankansha.me/rss/GEYDCOBUPRRWGMDBGFSDIZTGMM2GCZRVMQZGGNRQHBRTQZLEME4TQYLFGJRDKZBTGI2TSZJUGM======

### [[CL]ã€ŠRobustness Gym: Unifying the NLP Evaluation Landscapeã€‹K Goel, N Rajani, J Vig, S Tan, J Wu, S Zheng, C Xiong, M Bansal, C RÃ© [Stanford Univers...](https://weibo.com/1402400261/JDeum3Fum)

### [ã€Šä»Šæ—¥å­¦æœ¯è§†é‡(2021.1.15)ã€‹ç½‘é¡µé“¾æ¥](https://weibo.com/1402400261/JDehlDxhy)

### [æ—©ï¼[å¤ªé˜³] #æ—©å®‰# [å›¾ç‰‡]](https://weibo.com/1402400261/JDegju5vw)

### [ã€I received first-ever donation on my open-source side project and it felt greatã€‘ç½‘é¡µé“¾æ¥ æˆ‘åœ¨å¼€æºæ–¹é¢çš„é¡¹ç›®ä¸­è·å¾—äº†é¦–æ¬¡ææ¬¾ï¼Œæ„Ÿè§‰å¾ˆæ£’ã€‚å¼€æºåœ°å€æ‰˜ç®¡...](https://weibo.com/1715118170/JDequr4dL)

### error read: http://www.bigdatainterview.com/feed/

### [æ•°å­¦æ˜¯äººç±»çš„å‘æ˜ï¼Œè¿˜æ˜¯å‘ç°ï¼Ÿ](https://daily.zhihu.com/story/9732029)

### [å¦‚æœå…¬å¸å¼€å§‹æ¨è¡Œè¶…é•¿æ—¶é—´åŠ ç­åˆ¶åº¦ï¼Œåº”å½“å¦‚ä½•è‡ªæ•‘ï¼Ÿ](https://daily.zhihu.com/story/9732026)

### [äººç±»ä¸ºä»€ä¹ˆä¼šå­œå­œä¸å€¦åœ°å»æ¢ç´¢æœªçŸ¥ï¼Ÿå¥½å¥‡å¿ƒçš„æ„ä¹‰æ˜¯ä»€ä¹ˆ?](https://daily.zhihu.com/story/9732019)

### [åœ°çƒä¸Šæœ‰å“ªäº›ç½•è§çš„è‡ªç„¶å¥‡è§‚ï¼Ÿ](https://daily.zhihu.com/story/9732018)

### [ä¸ºä»€ä¹ˆå¤æ—¶ä¸œå—äºšéƒ½æ˜¯å°åº¦åŒ–è€Œéæ±‰åŒ–ï¼Ÿ](https://daily.zhihu.com/story/9732012)

### [çæ‰¯ Â· å¦‚ä½•æ­£ç¡®åœ°åæ§½](https://daily.zhihu.com/story/9732030)

### [CrateDbåœ¨æºç¨‹æœºç¥¨BIçš„å®è·µ](https://www.infoq.cn/article/pym3FDYeOz0oVbPFhjED)

### [[LG]ã€ŠFast convolutional neural networks on FPGAs with hls4mlã€‹T Aarrestad, V Loncar, M Pierini, S Summers, J Ngadiuba, C Petersson, H Linander, Y Iiy...](https://weibo.com/1402400261/JDeHqgOLJ)

### [[CV]ã€ŠBig Self-Supervised Models Advance Medical Image Classificationã€‹S Azizi, B Mustafa, F Ryan, Z Beaver, J Freyberg, J Deaton, A Loh, A Karthikesa...](https://weibo.com/1402400261/JDeDlkjND)

### [ã€Building a simple Web server with arsd CGI framework in D programming languageã€‘ç½‘é¡µé“¾æ¥ ä»¥Dç¼–ç¨‹è¯­è¨€ä½¿ç”¨arsd CGIæ¡†æ¶æ„å»ºç®€å•çš„WebæœåŠ¡å™¨ã€‚ç½‘è·¯å†·çœ¼æŠ€...](https://weibo.com/1715118170/JDf1rmYYH)

### [ã€How to Keep Up to Date with Web Developmentã€‘ç½‘è·¯å†·çœ¼æŠ€æœ¯åˆ†äº« #ç§‘æŠ€æš–å¿ƒå­£# [å›¾ç‰‡][å›¾ç‰‡]](https://weibo.com/1715118170/JDeOQlEkr)

### [ã€What Is a Gamma Squeeze in the Context of Stock Trading?ã€‘ç½‘é¡µé“¾æ¥ è‚¡ç¥¨äº¤æ˜“ä¸­çš„ä¼½ç›æŒ¤å‹æ˜¯ä»€ä¹ˆï¼Ÿ [å›¾ç‰‡]](https://weibo.com/1715118170/JDeCIDPds)

### error read: http://www.bigdatainterview.com/feed/

### [é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ](https://qingniantuzhai.com/qing-nian-tu-zhai-0115-3/)

 <!--kg-card-begin: markdown--><img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://qingniantuzhai.com/content/images/2021/01/897c4370gy1gmn8l2yuapj20zk0qoqv5.jpg" /><p>ã€1ã€‘å¤©ä¸‹æ­¦åŠŸï¼Œå”¯å¿«ä¸ç ´<br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://wx4.sinaimg.cn/mw1024/00893JKXly1gmnd5wmlsmg307s0dunpe.gif" /></p>
<p>ã€2ã€‘åå­—å¤ªé•¿æƒ¹çš„ç¥¸<br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://wx1.sinaimg.cn/mw600/006ahuzrly1gmncjwtsf7j30st15hgpq.jpg" /></p>
<p>ã€3ã€‘ç‰¹æœ—æ™®æ‰æ˜¯æœ€æ£’çš„æ€»ç»Ÿ<br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://wx2.sinaimg.cn/mw600/005PTIKbly1gmmck96vzbj30j60lw1ex.jpg" /></p>
<p>ã€4ã€‘å®ƒåƒè®©å®ƒåƒ<br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://wx4.sinaimg.cn/mw600/00893JKXly1gmnbi75ngkj30f00sz11s.jpg" /></p>
<p>ã€5ã€‘å¯ä¸æ•¢å¼€è¿™è½¦å»æ‰«å¢“<br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://wx4.sinaimg.cn/mw1024/00893JKXly1gmnais1e3ag30a00bc4qs.gif" /></p>
<p>ã€6ã€‘å˜²è®½æ»¡åˆ†äº†<br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://wx4.sinaimg.cn/mw600/0035UptZly1gmn56q4jw9j60kb0kbjtr02.jpg" /></p>
<p>ã€7ã€‘æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ<br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://wx3.sinaimg.cn/mw600/897c4370gy1gmn8l2yuapj20zk0qoqv5.jpg" /></p>
<p>ã€8ã€‘è¢«è‡ªå·±å“è’™äº†<br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://wx3.sinaimg.cn/mw1024/00893JKXly1gmn8deamwxg30aa055b29.gif" /></p>
<p>ã€9ã€‘æ—¶é—´çš„ç—•è¿¹<br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://wx1.sinaimg.cn/mw600/00893JKXly1gmn7w8mq6bj31400u01kx.jpg" /><br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://wx2.sinaimg.cn/mw600/00893JKXly1gmn7vzw2iej31400u0b1x.jpg" /></p>
<p>ã€10ã€‘å‹ä¸Šå…¨éƒ¨ç‹—ç²®çš„ç‹—å­<br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://wx2.sinaimg.cn/mw1024/006ahuzrgy1gmmqwgo4jig309y0e1he8.gif" /></p>
<p>ã€11ã€‘å­¦è‹±è¯­<br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://wx3.sinaimg.cn/mw600/00893JKXly1gmnhronwwtj319o0jwb29.jpg" /></p>
<p>ã€12ã€‘ä¸ç”±è‡ªä¸»çš„æäº†ä¸€ä¸‹ç¼¸<br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://wx3.sinaimg.cn/mw1024/00893JKXly1gmnezm3wm7g30730cinpe.gif" /></p>
<p>ã€13ã€‘å£«å¯æ€ä¸å¯è¾±<br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://wx4.sinaimg.cn/mw1024/00893JKXly1gmnezegudwg304c07kx6p.gif" /></p>
<p>ã€14ã€‘è‚‰æ£’è¿›å»ï¼Œæ´»çŒªå‡ºæ¥<br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://wx1.sinaimg.cn/mw600/00893JKXly1gmnet4y8fhj30g40o5jtl.jpg" /></p>
<p>ã€15ã€‘ä¸€å®¶å­è¢«ä¸€ä¸ªå¤–å§“äººæ¬ºè´Ÿ<br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://wx1.sinaimg.cn/mw1024/00893JKXly1gmndmcfcsxg304c06nu0z.gif" /></p>
<p>ã€16ã€‘è¯´å‡ºæ¥ä½ å¯èƒ½ä¸ä¿¡ æˆ‘è¢«åºŠç¼ ä¸Šäº†<br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://wx2.sinaimg.cn/mw1024/00893JKXly1gmn41azvc9g30b4089x6p.gif" /></p>
<p>ã€17ã€‘æœç‹—æœå‡ºæ¥æ˜¯ç‹—ï¼Œè¿™å¾ˆåˆç†<br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://wx1.sinaimg.cn/mw600/00893JKXly1gmn2q8ob7hj30f00mk0tc.jpg" /></p>
<p>ã€18ã€‘é“æ£æƒŠé¸¿èµ·ï¼Œè›‹ç¢äº†æ— ç—•<br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://ww1.sinaimg.cn/mw1024/538add77gy1gmn1203zj3g20a007i4qp.gif" /></p>
<p>ã€19ã€‘æ¥ï¼Œä½ è¦çš„å†æµªä¸€ç‚¹<br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://ww1.sinaimg.cn/mw1024/538add77gy1gmn10rg3hcg205l05ub29.gif" /></p>
<p>ã€20ã€‘é‚£ä¸ªå¹²éƒ¨ç»å¾—ä½è¿™ç§è€ƒéªŒ<br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://wx2.sinaimg.cn/mw1024/00893JKXly1gmmztksti7g30f40dee84.gif" /></p>
<p>ã€21ã€‘æˆ’è‰²å§é˜´è°‹<br />
<img alt="é’å¹´å›¾æ‘˜0115ï¼æ­£ä¹‰åœ¨ç¾è‰²é¢å‰ä¸å€¼ä¸€æ" src="https://wx3.sinaimg.cn/mw600/00893JKXly1gmmz1oi5efj30u00y4416.jpg" /></p>
<!--kg-card-end: markdown-->

### [Daily Hacker News for 2021-01-14](https://www.daemonology.net/hn-daily/2021-01-14.html)

### [Angelåœ¨TI-ONEæœºå™¨å­¦ä¹ å¹³å°ä¸Šçš„åº”ç”¨](https://www.infoq.cn/article/4R9hLPiiABCwzcEhVYsS)

### [#å°Qåˆ†äº«# ã€Šå¦‚ä½•åˆ©ç”¨TensorFlow Hub è®©BERTå¼€å‘æ›´ç®€å•ï¼Ÿã€‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼ŒBERT å’Œå…¶ä»– Transformer ç¼–ç å™¨æ¶æ„éƒ½éå¸¸æˆåŠŸï¼Œæ— è®ºæ˜¯æ¨è¿›å­¦æœ¯åŸºå‡†çš„æŠ€æœ¯æ°´å¹³...](https://weibo.com/1746173800/JDfpnjAKw)

### [ã€My personal wishlist for a decentralized social networkã€‘ç½‘é¡µé“¾æ¥ æˆ‘å¯¹å»ä¸­å¿ƒåŒ–ç¤¾äº¤ç½‘ç»œçš„ä¸ªäººæ„¿æœ›æ¸…å•ã€‚ ç½‘è·¯å†·çœ¼æŠ€æœ¯åˆ†äº« #ç§‘æŠ€æš–å¿ƒå­£# [å›¾ç‰‡]](https://weibo.com/1715118170/JDfpz9VPt)

### [ã€Beaker Browser â€“ experimental peer to peer web browserã€‘ç½‘é¡µé“¾æ¥ Beaker æµè§ˆå™¨â€“å®éªŒæ€§å¯¹ç­‰ç½‘ç»œæµè§ˆå™¨ã€‚ç½‘è·¯å†·çœ¼æŠ€æœ¯åˆ†äº« #ç§‘æŠ€æš–å¿ƒå­£# [å›¾ç‰‡]](https://weibo.com/1715118170/JDfdcirTn)

### [è½¬å‘å¾®åš - è½¬å‘ @ç½‘è·¯å†·çœ¼:&ensp;#å†·çœ¼èµ ä¹¦ç¦åˆ©# è½¬å‘èµ ä¹¦# ç½‘è·¯å†·çœ¼è”åˆ@åç« å›¾ä¹¦ @åç« è®¡ç®—æœºç§‘å­¦ é€å‡º 5 æœ¬ã€ŠEChartsæ•°æ®å¯è§†åŒ–ã€‹ï¼Œæˆªè‡³ 1 æœˆ 20 æ—¥ï¼Œè½¬å‘æ­¤...](https://weibo.com/1715118170/JDf76lfLV)

### [è½¬å‘ - è½¬å‘ @ç½‘è·¯å†·çœ¼:&ensp;ã€I received first-ever donation on my open-source side project and it felt greatã€‘ç½‘é¡µé“¾æ¥ æˆ‘åœ¨å¼€æºæ–¹é¢çš„é¡¹ç›®ä¸­è·å¾—äº†é¦–æ¬¡...](https://weibo.com/1642628345/JDfiWnAtC)

### [æ´¾æ—©æŠ¥ï¼šä¸‰æ˜Ÿå‘å¸ƒ Galaxy S21 ç³»åˆ—æ‰‹æœºã€è’¸æ±½å¹³å°ï¼ˆSteamï¼‰å³å°†ç™»é™†ä¸­å›½](https://sspai.com/post/64578)

### error read: https://rsshub.app/gov/ndrc/xwdt

### [æ‹œç™»çš„ç»æµå›¢é˜Ÿå°†å¦‚ä½•åˆ¶å®šä¸­å›½æ”¿ç­–ï¼Ÿ](http://www.ftchinese.com/story/001091021)

### [è¾¾é‡Œå¥¥ï¼šä¸­å›½å°†äº‰å¤ºä¸–ç•Œé‡‘èä¸­å¿ƒåœ°ä½](http://www.ftchinese.com/story/001090967)

### [ä½ ä»¬å®¶å°å­©ç™»è®°çš„å†œå†è¿˜æ˜¯å›½å†ï¼Ÿ](https://www.v2ex.com/t/745047)

### [å¤©å¤©å†™è„šæœ¬æ­ç¯å¢ƒï¼Œè¯¥ä¸è¯¥è¾èŒ](https://www.v2ex.com/t/744835)

### [æ··åˆåŸºç¡€è®¾æ–½ä¸‹ï¼ŒæœåŠ¡ç½‘æ ¼ï¼ˆService Meshï¼‰å¦‚ä½•å¯¹åº”ç”¨è¿›è¡Œç»Ÿä¸€ç®¡ç†](https://www.infoq.cn/article/4Gr7CGucVKGhOvjoBgEI)

### [æ¢ç´¢ Vue.js å“åº”å¼åŸç†](https://segmentfault.com/a/1190000038921922)

### [ã€ŠçŒ«ã€çˆ±å› æ–¯å¦å’Œå¯†ç å­¦ï¼šæˆ‘ä¹Ÿèƒ½çœ‹æ‡‚çš„é‡å­é€šä¿¡ã€‹ä»Šæ—¥å¼€å¥–ï¼Œæ¬¢è¿å‚ä¸ï½ - è½¬å‘ @çˆ±å¯å¯-çˆ±ç”Ÿæ´»:&ensp;#æŠ½å¥–##èµ ä¹¦#æ´»åŠ¨æ±‡æ€»ï¼Œå‚ä¸è¯·è½¬å‘åŸå¾®åšï¼šã€ŠçŒ«ã€çˆ±å› æ–¯å¦...](https://weibo.com/1402400261/JDfGmtJtj)

### [ã€Why We Disable Linux's THP Feature for Databasesã€‘ç½‘é¡µé“¾æ¥ ä¸ºä»€ä¹ˆæˆ‘ä»¬ç¦ç”¨Linuxçš„THPæ•°æ®åº“åŠŸèƒ½ã€‚ [å›¾ç‰‡]](https://weibo.com/1715118170/JDgceDcvZ)

### [ã€75% of the original Disney+ executive team is no longer at Disneyã€‘ç½‘é¡µé“¾æ¥ æœ€åˆçš„è¿ªå£«å°¼+æ‰§è¡Œå›¢é˜Ÿä¸­æœ‰75ï¼…ä¸å†åœ¨è¿ªå£«å°¼å·¥ä½œã€‚](https://weibo.com/1715118170/JDfNXttT7)

### [è½¬å‘ - è½¬å‘ @æ­ªæ€w4Wise:&ensp;ä¸€ä¸ªsuper()æŒ–å‡ºæ¥çš„å¤§é‡ä¿¡æ¯Pythonå†…æ ¸å¼€å‘è€…çš„è®²è§£[rhettinger (Raymond Hettinger)](ç½‘é¡µé“¾æ¥)Googleå¼€å‘è€… laike9mçš„è§£æ[ç†...](https://weibo.com/1642628345/JDfZ4cHss)

### [æ€•äº†æ€•äº† [å›¾ç‰‡]](https://weibo.com/1642628345/JDfNqnPUD)

### [ç§‘æŠ€çˆ±å¥½è€…å‘¨åˆŠï¼ˆç¬¬ 142 æœŸï¼‰ï¼š2020å¹´æ‰æ˜¯21ä¸–çºªå…ƒå¹´](http://www.ruanyifeng.com/blog/2021/01/weekly-issue-142.html)

### error read: http://www.waerfa.com/feed

### [ç™¾ç‚¼æ™ºèƒ½å®Œæˆ1äº¿å…ƒAè½®èèµ„ï¼Œæ·±è€•B2Bè¥é”€è‡ªåŠ¨åŒ–èµ›é“](https://www.jiqizhixin.com/articles/2021-01-15-2)

 

### error read: http://www.waerfa.com/feed

### error read: http://www.13775.org/feed/

### error read: http://www.bigdatainterview.com/feed/

### error read: http://120.53.237.72:1200/segmentfault/channel/ai

### error read: http://www.jintiankansha.me/rss/GEYDCOBUPRRWGMDBGFSDIZTGMM2GCZRVMQZGGNRQHBRTQZLEME4TQYLFGJRDKZBTGI2TSZJUGM======

### error read: http://www.jintiankansha.me/rss/GEYDCNRWPQ2TIZJWGJQTINLEMUYGGNRXMVRDSNZSG4ZDMNJQMU4WEMBZGAYTMMZQMEZGCMZZGE======

### [1æœˆ15æ—¥æ–°é—»èŒ¶æ³¡Fan](https://www.cfan.com.cn/2021/0115/134758.shtml)

### [ã€How the Canadian Tech Scene Encourages Finite Gameplayã€‘ç½‘é¡µé“¾æ¥ åŠ æ‹¿å¤§ç§‘æŠ€ç•Œå¦‚ä½•é¼“åŠ±æœ‰é™çš„æ¸¸æˆç©æ³•ï¼Ÿ](https://weibo.com/1715118170/JDgACpKKF)

### [//@å´æ½æµ :æœ‰é“ç† - è½¬å‘ @ä¼ æ’­å­¦è€ƒç ”å°±æ‰¾ç“¦æ´›ä½³:&ensp;ã€ä¸ºä»€ä¹ˆæˆ‘è§‰å¾— #ä¹Œåˆä¹‹ä¼—# å¯èƒ½æ˜¯æœ¬åä¹¦ã€‘ä¼ æ’­å­¦ï¼Œä¹ƒè‡³ç¤¾ä¼šåœˆçš„â€œç ´åœˆâ€ï¼Œå¦‚ä»Šæ˜¯è¶Šæ¥è¶Šå¤šäº†ã€‚æ˜¨å¤©ï¼Œä¸€æœ¬...](https://weibo.com/1642628345/JDgl34ECY)

### [å¹´åº¦å¾æ–‡ï½œèŠèŠæˆ‘åœ¨ç¾å›½å¦‚ä½•é¢†å…»ä¸€åªæ±ªæ˜Ÿäºº](https://sspai.com/post/64527)

### [6 â€“ Android æ‰‹æœºå²ä¸Šæœ€ 6 çš„åº”ç”¨](https://www.appinn.com/6-six-app-for-android/)

 <p>6 æ˜¯ä¸€æ¬¾éå¸¸ 6 çš„åº”ç”¨ï¼Œå ªç§° Android æ‰‹æœºå²ä¸Šæœ€ 6 çš„åº”ç”¨ï¼Œå®ƒå”¯ä¸€çš„åŠŸèƒ½ï¼Œå°±æ˜¯åœ¨æ‰‹æœºå±å¹•ä¸Šå±•ç¤ºä¸€ä¸ªæ•°å­— 6ï¼Œç‚¹å‡»å±å¹•è¿˜èƒ½æ›´æ¢ä¸åŒçš„é¢œè‰²ï¼ŒçœŸæ˜¯å¤ª 666 äº†ã€‚@<a class="rank-math-link" href="https://www.appinn.com/6-six-app-for-android/">Appinn</a></p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img alt="6 - Android æ‰‹æœºå²ä¸Šæœ€ 6 çš„åº”ç”¨" src="https://img3.appinn.net/images/202101/666.jpg!o" title="6 - Android æ‰‹æœºå²ä¸Šæœ€ 6 çš„åº”ç”¨ 1" /></figure></div>



<p>ç¬¬ä¸€æ¬¡æ‰“å¼€ 6ï¼Œé’å°è›™å¿ä¸ä½ç¬‘ï¼Œä¸çŸ¥é“ä¸ºä»€ä¹ˆï¼Œç‚¹äº†å‡ æ¬¡åˆåœ¨å¤§ç¬‘ï¼Œå¼€å¿ƒï¼Œå°±æ˜¯ 6ã€‚</p>



<p>æ€ä¹ˆè¯´ï¼Œé’å°è›™ä¹Ÿç®—æ˜¯è§è¯†è¿‡æˆç™¾ä¸Šåƒ App çš„ç”¨æˆ·ï¼Œåœ¨çœ‹äº† 6 ä¹‹åï¼Œè¿˜æ˜¯è§‰å¾— 6ï¼ŒçœŸæ˜¯ 6ã€‚</p>



<h2>Android æ‰‹æœºå²ä¸Šæœ€ 6 çš„åº”ç”¨</h2>



<p>æ˜¯çš„ï¼Œä½ èƒ½æ‰¾åˆ°æ¯” 6 è¿˜ 6 çš„åº”ç”¨ä¹ˆï¼Ÿ</p>



<p>æ¥çœ‹åŠ¨ç”»æ¼”ç¤º <img alt="ğŸ™ˆ" class="wp-smiley" src="https://s.w.org/images/core/emoji/13.0.1/72x72/1f648.png" style="height: 1em;" /></p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="Android æ‰‹æœºå²ä¸Šæœ€ 6 çš„åº”ç”¨" height="-449" src="https://img3.appinn.net/images/202101/666-appinn.gif!o" title="6 - Android æ‰‹æœºå²ä¸Šæœ€ 6 çš„åº”ç”¨ 2" width="-231" /></figure></div>



<p>æ‰“å¼€ 6 ä¹‹åï¼Œå°±æ˜¾ç¤º 6 åœ¨å±å¹•ä¸Šï¼Œç‚¹å‡»å±å¹•å¯ä»¥æ›´æ¢é¢œè‰²ï¼Œå°±æ²¡æœ‰å…¶ä»–åŠŸèƒ½äº†ï¼Œå¾ˆ 6ã€‚</p>



<p>å¼€å‘è€…ä¹Ÿæ˜¯å®åœ¨ï¼Œåœ¨åº”ç”¨ä»‹ç»ä¸­å†™ç€ï¼š</p>



<p>è¿™æ¬¾åº”ç”¨å°†ä¸ºä½ æä¾›æ¯”ä»¥å¾€æ›´å¤šçš„å¿«ä¹ã€‚ä½ å°†è¢«æ•°å­— 6 çš„çº¯æ´æœ¬è´¨æ‰€æ¿€åŠ±ï¼Œå¹¶å†ä¸€æ¬¡é‡æ¸©ä½ æœ€å–œæ¬¢çš„æ—¶åˆ»ï¼Œå› ä¸ºä½ æ¬¢æ¬£é¼“èˆï¼Œå¾—åˆ°æ›´å¤šçš„çµæ„Ÿã€‚6.</p>



<p>å•Šï¼Œé’å°è›™å®Œå…¨è¢« 6 æ„ŸæŸ“äº†ï¼Œè¯·å‰å¾€ <a class="rank-math-link" href="https://play.google.com/store/apps/details?id=six.six" rel="noopener" target="_blank">Google Play</a> å…è´¹å®‰è£…ï¼Œæ— æ³•è®¿é—® Play å•†åº—çš„åŒå­¦å¯ç‚¹å‡»ä¸‹é¢çš„æŒ‰é’®å‰å¾€ç½‘ç›˜æ¬è¿ï¼š</p>



<div align="center"><a href="https://d.appinn.com/6-six-app-for-android/" rel="noopener" target="view_window"><img alt="6 - Android æ‰‹æœºå²ä¸Šæœ€ 6 çš„åº”ç”¨ 1" src="https://img3.appinn.net/images/201507/down.png" title="6 - Android æ‰‹æœºå²ä¸Šæœ€ 6 çš„åº”ç”¨ 3" /></a></div>
<hr /><h2>ç›¸å…³é˜…è¯»</h2><ul><li><a href="https://www.appinn.com/six-for-ios/" rel="bookmark" title="Permanent Link: Six! &#8211; çˆ±æ¶ˆé™¤å’Œçˆ±å¹³è¡¡åˆä½“äº†[iPad/iPhone]">Six! &#8211; çˆ±æ¶ˆé™¤å’Œçˆ±å¹³è¡¡åˆä½“äº†[iPad/iPhone]</a></li><li><a href="https://www.appinn.com/leo-share-apps-ios/" rel="bookmark" title="Permanent Link: @Leo&#038; : è‡ªç”¨ç²¾å“ App æ•´ç†ä¸æ¨è iOS ç‰ˆ">@Leo&#038; : è‡ªç”¨ç²¾å“ App æ•´ç†ä¸æ¨è iOS ç‰ˆ</a></li><li><a href="https://www.appinn.com/six-accountbook/" rel="bookmark" title="Permanent Link: çº¯è®°è´¦ &#8211; ä¸€æ¬¾ Android ä¸Šçš„ã€Œçº¯è®°è´¦ã€åº”ç”¨">çº¯è®°è´¦ &#8211; ä¸€æ¬¾ Android ä¸Šçš„ã€Œçº¯è®°è´¦ã€åº”ç”¨</a></li><li><a href="https://www.appinn.com/the-iphone-storage-almost-full/" rel="bookmark" title="Permanent Link: iPhone å®¹é‡ä¸è¶³çš„æ—¶å€™ï¼Œè¿˜èƒ½è¿™æ ·ï¼">iPhone å®¹é‡ä¸è¶³çš„æ—¶å€™ï¼Œè¿˜èƒ½è¿™æ ·ï¼</a></li><li><a href="https://www.appinn.com/sfufoet-ios-apps/" rel="bookmark" title="Permanent Link: åº”ç½‘æ˜“åº”ç”¨çš„é‚€è¯·ï¼Œæ¨èä¸€äº›å°ä¼—çš„ iOS apps">åº”ç½‘æ˜“åº”ç”¨çš„é‚€è¯·ï¼Œæ¨èä¸€äº›å°ä¼—çš„ iOS apps</a></li></ul><hr />
<a href="http://www.appinn.com/copyright/?utm_source=feeds&amp;utm_medium=copyright&amp;utm_campaign=feeds" title="ç‰ˆæƒå£°æ˜">&#169;</a>2019 é’å°è›™ for <a href="http://www.appinn.com/?utm_source=feeds&amp;utm_medium=appinn&amp;utm_campaign=feeds" title="æœ¬æ–‡æ¥è‡ªå°ä¼—è½¯ä»¶">å°ä¼—è½¯ä»¶</a> | <a href="http://www.appinn.com/join-us/?utm_source=feeds&amp;utm_medium=joinus&amp;utm_campaign=feeds" title="åŠ å…¥å°ä¼—è½¯ä»¶">åŠ å…¥æˆ‘ä»¬</a> | <a href="https://meta.appinn.com/c/faxian/?utm_source=feeds&amp;utm_medium=contribute&amp;utm_campaign=feeds" rel="noopener" target="_blank" title="ç»™å°ä¼—è½¯ä»¶æŠ•ç¨¿">æŠ•ç¨¿</a> | <a href="http://www.appinn.com/feeds-subscribe/?utm_source=feeds&amp;utm_medium=feedsubscribe&amp;utm_campaign=feeds" target="_blank" title="å¯ä»¥åˆ†ç±»è®¢é˜…å°ä¼—ï¼ŒWindows/MAC/æ¸¸æˆ"><font color="red">è®¢é˜…æŒ‡å—</font></a><br /> 3659b075e72a5b7b1b87ea74aa7932ff <br />
<a href="https://www.appinn.com/6-six-app-for-android/#comments" title="to the comments">ç‚¹å‡»è¿™é‡Œç•™è¨€ã€å’ŒåŸä½œè€…ä¸€èµ·è¯„è®º</a>

### [2020å¹´æˆ¿ä»·æ¶¨å¹…æ¦œå‡ºç‚‰ï¼š43åŸæˆ¿ä»·ä¸Šæ¶¨ï¼Œæ·±åœ³é¢†å…ˆï¼Œä½ å®¶æˆ¿å­æ˜¯æ¶¨è¿˜æ˜¯è·Œï¼Ÿ](https://m.21jingji.com/article/20210115/herald/96103ba14c6dba013fa45f216779cd8b.html)

### [ç¢°åˆ°æœ‰äººå…‹éš†è‡ªå·±åšå®¢è¯¥æ€ä¹ˆåŠï¼Ÿ](https://www.v2ex.com/t/745097)

### [955 ä¹Ÿæ²¡æ³• WLB](https://www.v2ex.com/t/745039)

### [å„ä½å¤§ä½¬ï¼Œè¯·æ•™ä¸€ä¸‹å¦‚ä½•åœ¨æ–­ç½‘çš„ç¯å¢ƒä¸‹åŒæ­¥ä»£ç ï¼Ÿ](https://www.v2ex.com/t/745005)

### [æŸç”·å­ç ”å‘ä¸Šä¼ â€œå¥åº·ç æ¼”ç¤ºâ€App è¢«æŠ“è·](https://linux.cn/article-13017-1.html?utm_source=rss&utm_medium=rss)

### [æ­å–œ@æXèŒLYM 1åç”¨æˆ·è·å¾—ã€åä¸ºFreeBuds ProçœŸæ— çº¿è“ç‰™è€³æœºã€‘ã€‚å¾®åšå®˜æ–¹å”¯ä¸€æŠ½å¥–å·¥å…·@å¾®åšæŠ½å¥–å¹³å° -é«˜çº§ç‰ˆå¯¹æœ¬æ¬¡æŠ½å¥–è¿›è¡Œç›‘ç£ï¼Œç»“æœå…¬æ­£æœ‰æ•ˆã€‚å…¬ç¤ºé“¾æ¥ï¼šå¾®åš...](https://weibo.com/1746173800/JDh0Qua16)

### [A16Z åˆä¼™äºº Peterï¼šå¼€æºï¼Œä»ç¤¾åŒºåˆ°å•†ä¸šåŒ–](https://segmentfault.com/a/1190000038990512)

### [å‡ ç¯‡è®ºæ–‡å®ç°ä»£ç ï¼šã€ŠImvotenet: Boosting 3d object detection in point clouds with image votesã€‹(CVPR 2020) GitHub:ç½‘é¡µé“¾æ¥ [fig1]ã€ŠA Multi-task Learnin...](https://weibo.com/1402400261/JDhiZcaM4)

### [æ­å–œ@è¦ç»§ç»­ç…§é¡¾ç€å†ä»£æ˜Ÿè¾° ç­‰3åç”¨æˆ·è·å¾—ã€ã€ŠçŒ«ã€çˆ±å› æ–¯å¦å’Œå¯†ç å­¦ã€‹ã€‘ã€‚å¾®åšå®˜æ–¹å”¯ä¸€æŠ½å¥–å·¥å…·@å¾®åšæŠ½å¥–å¹³å° å¯¹æœ¬æ¬¡æŠ½å¥–è¿›è¡Œç›‘ç£ï¼Œç»“æœå…¬æ­£æœ‰æ•ˆã€‚å…¬ç¤ºé“¾æ¥ï¼šå¾®...](https://weibo.com/1402400261/JDgZEld7k)

### [ã€Signal Fork with WhatsApp Migrationã€‘ç½‘é¡µé“¾æ¥ å¸¦WhatsAppè¿ç§»çš„Signal åˆ†æ”¯ã€‚ [å›¾ç‰‡]](https://weibo.com/1715118170/JDhns2AG9)

### error read: http://www.waerfa.com/feed

### error read: https://sspai.com/feed

### error read: http://www.waerfa.com/feed

### error read: http://www.jintiankansha.me/rss/GE2DK7BQMI2DKYLDGM2DMMBVMY3WKY3FMQ4TMY3FME4DOOJQGBSWCNLDMZRTEYLGGBRTGZQ=

### error read: http://www.jintiankansha.me/rss/GEYDCOBUPRRWGMDBGFSDIZTGMM2GCZRVMQZGGNRQHBRTQZLEME4TQYLFGJRDKZBTGI2TSZJUGM======

### [è¿™ä¹ˆå¤šèŒ…å°æ˜¯å–ç»™è°äº†](https://www.v2ex.com/t/745122)

### [å¯ä»¥è¯´éå¸¸çš„ç„¦è™‘äº†ï¼Œæ±‚è§£å†³æ–¹æ¡ˆ](https://www.v2ex.com/t/744971)

### [Twitteræ°¸ä¹…æ€§çš„å¤¹äº†Sci-Hubçš„â€œå®˜æ–¹â€å¸æˆ·](http://jandan.net/p/108351)

### [è¯·æ•™ä¸€ä¸ªrustè°ƒç”¨CåŠ¨æ€åº“å«Cç»“æ„ä½“çš„é—®é¢˜](https://rustcc.cn/article?id=e09fe057-2caa-483b-b72c-baca380ce9e5)

### [é«˜è¾¾200ä¸ªåº”ç”¨ï¼Œè¿‘8000ä¸ªå®ä¾‹çš„å·¥è¡ŒMySQLè½¬å‹å®è·µ](https://www.infoq.cn/article/EQRMLltiKaEfGKSXTtkE)

### [ç¾ç±åäººMITæ•™æˆé™ˆåˆšè¢«æ•ï¼šæˆ–é¢ä¸´æœ€é«˜20å¹´ç›‘ç¦](https://www.infoq.cn/article/zBOvrsegmsDuQOcYJgPW)

### [ã€Messenger Comparisons â€“ Threema, Signal, Telegram and WhatsAppã€‘ç½‘é¡µé“¾æ¥ Threemaï¼ŒSignalï¼ŒTelegramå’ŒWhatsAppç­‰4ä¸ªMessengeræ¯”è¾ƒã€‚ [å›¾ç‰‡]](https://weibo.com/1715118170/JDhLLClFP)

### error read: http://www.waerfa.com/feed

### error read: https://sspai.com/feed

### error read: http://www.waerfa.com/feed

### error read: https://szpzs.oschina.io/atom.xml

### error read: http://120.53.237.72:1200/zhihu/zhuanlan/chicken-life

### error read: http://www.bigdatainterview.com/feed/

### [ä¸çŸ¥é“è“„æ°´æ± æŠ½æ ·ç®—æ³•ï¼Ÿé‚£å°±è¿›æ¥çœ‹çœ‹å§~](https://segmentfault.com/a/1190000038991899)

### error read: http://www.jintiankansha.me/rss/GEYDCOBUPRRWGMDBGFSDIZTGMM2GCZRVMQZGGNRQHBRTQZLEME4TQYLFGJRDKZBTGI2TSZJUGM======

### error read: http://www.jintiankansha.me/rss/GEYDCNRWPQ2TIZJWGJQTINLEMUYGGNRXMVRDSNZSG4ZDMNJQMU4WEMBZGAYTMMZQMEZGCMZZGE======

### error read: https://rsshub.app/telegram/channel/tgchinanews

### [å¯¹ä¸‰æ˜Ÿ s21 ç³»åˆ—çš„è¯„ä»·](https://www.v2ex.com/t/745099)

### [æ³¨é”€å¸è½½æ·˜å®æ”¯ä»˜å®åèƒ½è¯·é˜¿é‡Œå…¬å¸åœ¨æ•°æ®åº“ä¸­åˆ é™¤ç”¨æˆ·ä¿¡æ¯å—ï¼Ÿ](https://www.v2ex.com/t/745092)

### error read: https://rust.cc/rss

### [#å†·çœ¼èµ ä¹¦ç¦åˆ©# è”åˆ @åšæ–‡è§†ç‚¹Broadview é€å‡º5æœ¬ã€ŠæŠ€æœ¯äººä¿®ç‚¼ä¹‹é“ï¼šä»ç¨‹åºå‘˜åˆ°ç™¾ä¸‡é«˜ç®¡çš„72é¡¹æŠ€èƒ½ã€‹ã€‚æˆªæ­¢ 1 æœˆ 22 æ—¥ï¼Œè½¬å‘æ­¤å¾®åšå¹¶å…³æ³¨ @ç½‘è·¯å†·çœ¼ èµ¢å–ã€‚å…¨æ–¹...](https://weibo.com/1715118170/JDi8Hd59o)

### [TWS é™å™ªè€³æœºã€Œè·³çº§ç”Ÿã€ï¼šå£°é˜” Liberty Air 2 Pro è®©æˆ‘æ‰‹ä¸Šçš„ AirPods Pro ä¸é¦™äº†](https://sspai.com/post/64566)

### [ä¹°äº†éšå¿ƒé£ N ä¸ªæœˆï¼Œè¿™æ˜¯æˆ‘ä»¬çš„ä½“éªŒæŠ¥å‘Š](https://sspai.com/post/64582)

### error read: http://www.waerfa.com/feed

### error read: http://www.jintiankansha.me/rss/GE2DK7BQMI2DKYLDGM2DMMBVMY3WKY3FMQ4TMY3FME4DOOJQGBSWCNLDMZRTEYLGGBRTGZQ=

### error read: http://www.jintiankansha.me/rss/GEYDCOBUPRRWGMDBGFSDIZTGMM2GCZRVMQZGGNRQHBRTQZLEME4TQYLFGJRDKZBTGI2TSZJUGM======

### [è¿«äºå…¬å¸è¦é’‰é’‰æ‰“å¡ï¼Œè¯·é—®æœ‰å•¥åŠæ³•è™šæ‹Ÿå®šä½é’‰é’‰æ‰“å¡å—ï¼Ÿæ‰‹æœºå·² root](https://www.v2ex.com/t/745189)

### [ğŸ‘‚ğŸ»ï¼šæ™šä¸Šå›å°åŒºï¼Œæœ‰ä¸ªç”·äººéª‘ç”µç“¶è½¦è½½ç€å„¿å­è·¯è¿‡ä¸€ä½ç¯å«å·¥ï¼Œå°å­©é—®ï¼šâ€œä¸ºä»€ä¹ˆè€äººå®¶è¦åšè¿™äº›å•Šï¼Ÿâ€](https://weibo.com/2816125940/JDiyX57dW)

### [ğŸ‘‚ğŸ»ï¼šæ ¡å›­é‡Œï¼Œè¾¹ä¸Šè·¯è¿‡ä¸¤ä¸ªå¥³ç”Ÿï¼Œå¬åˆ°å…¶ä¸­ä¸€ä¸ªæƒŠè®¶åœ°è¯´ï¼šâ€œæˆ‘ä»¬å°æ—¶å€™éƒ½æ˜¯çœ‹å¥¥ç‰¹æ›¼çš„æ­£ä¹‰ï¼Œä½ æ€ä¹ˆå°æ—¶å€™å°±çœ‹ä»–èº«æå“‡ï¼â€](https://weibo.com/2816125940/JDiwlczBj)

### [ğŸ‘‚ğŸ»ï¼šåˆä¼‘æ—¶ä¸¤ä¸ªç”·åŒäº‹åœ¨èŠå¤©ï¼Œå…¶ä¸­ä¸€ä¸ªè¯´ï¼šâ€œæˆ‘ä¹Ÿæƒ³è¦æœ‰ä¸€ä¸ªå¥½çš„çš®å›Šï¼Œè®©äººçœ‹åˆ°æˆ‘éƒ½æµå£æ°´ã€‚â€](https://weibo.com/2816125940/JDivltc3Y)

### [ğŸ‘‚ğŸ»ï¼šç‰™ç§‘æ²»ç–—å®¤ï¼Œä¸€ä¸ªäº”å…­åå²çš„ç”·äººèººåœ¨è¯Šä½ä¸Šï¼Œå¹äº†å£æ°”ã€‚â€œå”‰ï¼Œæˆ‘å¤ªä¸å‹‡æ•¢äº†ã€‚â€](https://weibo.com/2816125940/JDiuYEAmC)

### [ğŸ‘‚ğŸ»ï¼šæˆ‘ä»¬é…’å§æ˜¯æ‹‰å§ï¼Œå¯¹é¢åˆšå·§æ˜¯é’™å§ï¼Œç­‰ç”µæ¢¯æ—¶å¬åˆ°ä¸¤ä¸ªGayçš„è®¨è®ºï¼šâ€œåˆšåˆšé‚£ä¸ªTçœŸçš„å¥½å¸…å•Šï¼â€](https://weibo.com/2816125940/JDiur1WAI)

### [ã€è¦å»ºç«‹â€œä¸­å›½ç‰ˆ GitHubâ€å—ï¼Ÿã€‘ç‚¹å‡»è§‚çœ‹ï¼šç½‘é¡µé“¾æ¥ å¾®è½¯æ¨å‡ºGitHubæš—é»‘æ¨¡å¼ï¼Œå¼•å‘ç¨‹åºå‘˜çƒ­è®®ï¼Œé‚£ä¹ˆè¦ä¸è¦å»ºç«‹ä¸€ä¸ªä¸­å›½çš„â€œGitHubâ€ï¼Ÿå¼€æºç•Œé¢†è¢–çº§äººç‰©å¯¹æ­¤ä»€ä¹ˆ...](https://weibo.com/1746173800/JDiyd2Clo)

### error read: http://www.waerfa.com/feed